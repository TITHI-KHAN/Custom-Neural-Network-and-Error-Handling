# -*- coding: utf-8 -*-
"""Lecture_05_1st_Code_Implementation_and_Fundamentals_of_Python_for_Deep_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dGzvaIZ6kYeTrxsKTOCzI-e0Kktg9Y_x

**Using TensorFlow and a Custom Model Class (customNeuralNetwork) to do Forward Propagation (OOP)**
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense

class customNeuralNetwork(tf.keras.Model):
    def __init__(self, n_input, n_hidden, n_output):
        super(customNeuralNetwork, self).__init__()
        self.hidden_layer = Dense(units=n_hidden, activation='sigmoid', input_shape=(5,3))
        self.output_layer = Dense(units=n_output, activation='sigmoid')
        #self.inpu = (input_shape=(n_input,))

    def call(self, inputs):
        hidden_activations = self.hidden_layer(inputs)
        output_activations = self.output_layer(hidden_activations)
        return output_activations


# Example usage:
n_input = 3  # Example number of input neurons
n_hidden = 5  # Example number of neurons in the hidden layer
n_output = 2  # Example number of neurons in the output layer

# Create an instance of the model
model = customNeuralNetwork(n_input, n_hidden, n_output)

# Generate some example input data
input_data = tf.random.normal(shape=(1, n_input))

# Perform forward propagation
output_data = model(input_data)

print("Input data:", input_data.numpy())
print("Output data:", output_data.numpy())

input_data = tf.random.poisson(lam=1.0, shape=(1, n_input))

input_data

class customNeuralNetwork(tf.keras.Model):
    def __init__(self, n_input, n_hidden, n_output):
        super(customNeuralNetwork, self).__init__()
        self.hidden_layer = Dense(units=n_hidden, activation='sigmoid', input_shape=(n_input,))
        self.output_layer = Dense(units=n_output, activation='sigmoid')

    def call(self, inputs):
        hidden_activations = self.hidden_layer(inputs)
        output_activations = self.output_layer(hidden_activations)
        return output_activations

n_input = 3  # Example number of input neurons
n_hidden = 5  # Example number of neurons in the hidden layer
n_output = 2  # Example number of neurons in the output layer

model = customNeuralNetwork(n_input, n_hidden, n_output)

input_data = tf.random.normal(shape=(1, n_input))

o = model(input_data)

o.shape

o.numpy()

"""**Without using TensorFlow to do Forward Propagation (OOP)**"""

import numpy as np

# Define the sigmoid activation function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

class customNeuralNetwork:
    def __init__(self, n_input, n_hidden, n_output):
        self.n_input = n_input
        self.n_hidden = n_hidden
        self.n_output = n_output

        # Initialize weights and biases for the hidden layer
        self.W1 = np.random.randn(n_hidden, n_input)
        self.b1 = np.random.randn(n_hidden, 1)

        # Initialize weights and biases for the output layer
        self.W2 = np.random.randn(n_output, n_hidden)
        self.b2 = np.random.randn(n_output, 1)

    def forward_propagation(self, x):
        # Step 1: Input to Hidden Layer
        z1 = np.dot(self.W1, x) + self.b1
        a1 = sigmoid(z1)

        # Step 2: Hidden to Output Layer
        z2 = np.dot(self.W2, a1) + self.b2
        a2 = sigmoid(z2)

        return a2

# Example usage:
n_input = 3  # Example number of input neurons
n_hidden = 5  # Example number of neurons in the hidden layer
n_output = 2  # Example number of neurons in the output layer

# Create an instance of the model
model = customNeuralNetwork(n_input, n_hidden, n_output)

# Generate some example input data
input_data = np.random.randn( n_input,1)

# Perform forward propagation
output_data = model.forward_propagation(input_data)

print("Input data:", input_data)
print("Output data:", output_data)

"""**Using TensorFlow and a Custom Forward Propagation Function to do Forward Propagation**"""

import tensorflow as tf

# Define the activation function
def activation_function(z):
    return tf.nn.sigmoid(z)  # Using sigmoid activation function

# Define forward propagation function
def forward_propagation(x, W1, b1, W2, b2):
    # Step 1: Input to Hidden Layer
    z1 = tf.matmul(W1, x) + b1
    a1 = activation_function(z1)

    # Step 2: Hidden to Output Layer
    z2 = tf.matmul(W2, a1) + b2
    a2 = activation_function(z2)

    return a2

# Example usage:
n_input = 3  # Number of input neurons
n_hidden = 4  # Number of neurons in the hidden layer
n_output = 2  # Number of neurons in the output layer

# Randomly initialize weights and biases for demonstration purposes
W1 = tf.random.normal(shape=(n_hidden, n_input))
b1 = tf.random.normal(shape=(n_hidden, 1))
W2 = tf.random.normal(shape=(n_output, n_hidden))
b2 = tf.random.normal(shape=(n_output, 1))

# Generate some example input data
x = tf.random.normal(shape=(n_input, 1))

# Perform forward propagation
output = forward_propagation(x, W1, b1, W2, b2)

print("Input data:")
print(x.numpy())
print("\nOutput data:")
print(output.numpy())

"""**Implementation of Forward Propagation in a Neural Network using TensorFlow**"""

import tensorflow as tf

# Define the sigmoid activation function
def sigmoid(z):
    return 1 / (1 + tf.exp(-z))

# Define the forward propagation function
def forward_propagation(x, W1, b1, W2, b2):
    # Step 1: Input to Hidden Layer
    z1 = tf.matmul(W1, x) + b1
    a1 = sigmoid(z1)

    # Step 2: Hidden to Output Layer
    z2 = tf.matmul(W2, a1) + b2
    a2 = sigmoid(z2)

    return a2

# Input data
x = tf.constant([[1.0], [2.0], [3.0]], dtype=tf.float32)

# Initialize weights and biases
W1 = tf.constant([[0.1, 0.2, 0.3],
                  [0.4, 0.5, 0.6],
                  [0.7, 0.8, 0.9],
                  [0.2, 0.3, 0.4]], dtype=tf.float32)
b1 = tf.constant([[0.1], [0.2], [0.3], [0.4]], dtype=tf.float32)
W2 = tf.constant([[0.2, 0.3, 0.4, 0.5],
                  [0.1, 0.2, 0.3, 0.4]], dtype=tf.float32)
b2 = tf.constant([[0.1], [0.2]], dtype=tf.float32)

# Perform forward propagation
output = forward_propagation(x, W1, b1, W2, b2)

print("Input data:")
print(x.numpy())
print("\nOutput data:")
print(output.numpy())

x = tf.constant([1.0, 2.0,3.0], dtype=tf.float32)

x

"""**Forward Propagation in a Neural Network using NumPy**"""

import numpy as np

# Define the sigmoid activation function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Define the forward propagation function
def forward_propagation(x, W1, b1, W2, b2):
    # Step 1: Input to Hidden Layer
    z1 = np.dot(W1, x) + b1
    a1 = sigmoid(z1)

    # Step 2: Hidden to Output Layer
    z2 = np.dot(W2, a1) + b2
    a2 = sigmoid(z2)

    return a2

# Input data
x = np.array([[1.0], [2.0], [3.0]])

# Initialize weights and biases
W1 = np.array([[0.1, 0.2, 0.3],
               [0.4, 0.5, 0.6],
               [0.7, 0.8, 0.9],
               [0.2, 0.3, 0.4]])
b1 = np.array([[0.1], [0.2], [0.3], [0.4]])
W2 = np.array([[0.2, 0.3, 0.4, 0.5],
               [0.1, 0.2, 0.3, 0.4]])
b2 = np.array([[0.1], [0.2]])

# Perform forward propagation
output = forward_propagation(x, W1, b1, W2, b2)

print("Input data:")
print(x)
print("\nOutput data:")
print(output)

"""**Forward Propagation through a Neural Network is implemented using TensorFlow**"""

import tensorflow as tf

# Define the sigmoid activation function
def sigmoid(z):
    return 1 / (1 + tf.exp(-z))

# Define forward propagation function
def forward_propagation(x, W1, b1, W2, b2):
    # Step 1: Input to Hidden Layer
    z1 = tf.matmul(W1, x) + b1
    a1 = sigmoid(z1)

    # Step 2: Hidden to Output Layer
    z2 = tf.matmul(W2, a1) + b2
    a2 = sigmoid(z2)

    return a2

# Define mean squared error function
def mean_squared_error(y_true, y_pred):
    return tf.reduce_mean(tf.square(y_true - y_pred))

# Input data
x = tf.constant([[1.0], [2.0], [3.0]], dtype=tf.float32)

# Initialize weights and biases
W1 = tf.constant([[0.1, 0.2, 0.3],
                  [0.4, 0.5, 0.6],
                  [0.7, 0.8, 0.9],
                  [0.2, 0.3, 0.4]], dtype=tf.float32)
b1 = tf.constant([[0.1], [0.2], [0.3], [0.4]], dtype=tf.float32)
W2 = tf.constant([[0.2, 0.3, 0.4, 0.5],
                  [0.1, 0.2, 0.3, 0.4]], dtype=tf.float32)
b2 = tf.constant([[0.1], [0.2]], dtype=tf.float32)

# Perform forward propagation
output = forward_propagation(x, W1, b1, W2, b2)

# Target output
# Here, you need to define your true output values
y_true = tf.constant([[0.5], [0.7]], dtype=tf.float32)

# Compute mean squared error
mse = mean_squared_error(y_true, output)

print("Mean Squared Error (MSE):", mse.numpy())

"""**Cross-Entropy Loss**"""

import tensorflow as tf

# Define the cross-entropy loss function
def cross_entropy_loss(y_true, y_pred):
    # Avoiding numerical instability by clipping values
    y_pred = tf.clip_by_value(y_pred, 1e-10, 1.0)
    return -tf.reduce_sum(y_true * tf.math.log(y_pred))

# Predicted output
y_pred = tf.constant([[0.704], [0.739]], dtype=tf.float32)

# Target output
y_true = tf.constant([[0.8], [0.9]], dtype=tf.float32)

# Compute cross-entropy loss
loss = cross_entropy_loss(y_true, y_pred)

print("Cross-Entropy Loss:", loss.numpy())

y_pred = tf.constant([[0.704], [0.704]], dtype=tf.float32)

y_pred = tf.clip_by_value(y_pred, 1e-10, 1.0)

import tensorflow as tf

t = tf.constant([[-10., -1., 0.], [-1., 200., 10.]])
t2 = tf.clip_by_value(t, clip_value_min=-1, clip_value_max=1)
print(t2.numpy())

"""**Mean Absolute Error (MAE)**"""

import tensorflow as tf

# Define the Mean Absolute Error (MAE) function
def mean_absolute_error(y_true, y_pred):
    return tf.reduce_mean(tf.abs(y_true - y_pred))

# Predicted output
y_pred = tf.constant([[0.704], [0.739]], dtype=tf.float32)

# Target output
y_true = tf.constant([[0.8], [0.9]], dtype=tf.float32)

# Compute Mean Absolute Error (MAE)
mae = mean_absolute_error(y_true, y_pred)

print("Mean Absolute Error (MAE):", mae.numpy())

y_pred = tf.constant([[0.704], [0.739]], dtype=tf.float32)

def mean_absolute_error(y_true, y_pred):
    return tf.reduce_mean(tf.abs(y_true - y_pred))

import tensorflow as tf

# Predicted output
y_pred = tf.constant([[0.704], [0.739]], dtype=tf.float32)

# Target output
y_true = tf.constant([[0.8], [0.9]], dtype=tf.float32)

"""**RMSE using built-in TensorFlow function**"""

rmse_tf = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))

"""**RMSE manually without using any function**"""

rmse_manual = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))

"""**MAPE using built-in TensorFlow function**"""

mape_tf = tf.reduce_mean(tf.abs((y_true - y_pred) / y_true)) * 100

"""**MAPE manually without using any function**"""

mape_manual = tf.reduce_mean(tf.abs((y_true - y_pred) / y_true)) * 100

"""**MASE manually without using any function**"""

naive_baseline_errors = tf.abs(y_true - tf.reduce_mean(y_true))
mase_manual = tf.reduce_mean(tf.abs(y_true - y_pred)) / tf.reduce_mean(naive_baseline_errors)

"""**MASE using TensorFlow operations**"""

import tensorflow as tf

# Predicted output
y_pred = tf.constant([[0.704], [0.739]], dtype=tf.float32)

# Target output
y_true = tf.constant([[0.8], [0.9]], dtype=tf.float32)

# Compute mean absolute error
mae = tf.reduce_mean(tf.abs(y_true - y_pred))

# Compute mean absolute scaled error
naive_baseline_errors = tf.abs(y_true - tf.reduce_mean(y_true))
mase = mae / tf.reduce_mean(naive_baseline_errors)

# Print MASE
print("MASE (Built-in):", mase.numpy())

"""**MSLE using built-in TensorFlow function**"""

msle_tf = tf.reduce_mean(tf.square(tf.math.log1p(y_true) - tf.math.log1p(y_pred)))

"""**MSLE manually without using any function**"""

msle_manual = tf.reduce_mean(tf.square(tf.math.log1p(y_true) - tf.math.log1p(y_pred)))

"""**Print results**"""

print("RMSE (Built-in):", rmse_tf.numpy())
print("RMSE (Manual):", rmse_manual.numpy())
print("MAPE (Built-in):", mape_tf.numpy())
print("MAPE (Manual):", mape_manual.numpy())
print("MASE (Manual):", mase_manual.numpy())
print("MASE (TensorFlow):", mase.numpy())
print("MSLE (Built-in):", msle_tf.numpy())
print("MSLE (Manual):", msle_manual.numpy())